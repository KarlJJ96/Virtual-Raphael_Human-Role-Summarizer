{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 transformers torch"
      ],
      "metadata": {
        "id": "V81xseeo1oZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34xEnYbt0e-1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import PyPDF2\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "def extract_and_summarize(file_path):\n",
        "    if not file_path.endswith('.pdf'):\n",
        "        return 'Please convert file to PDF.'\n",
        "\n",
        "    # Open the PDF file\n",
        "    with open(file_path, 'rb') as file:\n",
        "        # Create a PDF reader object\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Extract text from each page\n",
        "        extracted_text = []\n",
        "        for page in reader.pages:\n",
        "            extracted_text.append(page.extract_text())\n",
        "\n",
        "    # Define start and end phrases\n",
        "    start_phrases = [\"recommendation\", \"lessons learned\", \"advice to planning authorities\"]\n",
        "    end_phrases = [\"reference\", \"appendix\", \"annex\", \"list of\", \"conclusion\", \"bibliography\", \"works cited\",\n",
        "                   \"introduction\", \"board member statements\", \"executive summary\", \"abbreviations and acronyms\"]\n",
        "\n",
        "    # Extract the relevant part of the document\n",
        "    extracted_part = []\n",
        "    found_start = False\n",
        "    for line in extracted_text:\n",
        "        if any(phrase in line.lower() for phrase in start_phrases):\n",
        "            found_start = True\n",
        "        if found_start:\n",
        "            extracted_part.append(line)\n",
        "            if any(phrase in line.lower() for phrase in end_phrases):\n",
        "                break\n",
        "\n",
        "    # Extract sentences containing specific keywords\n",
        "    keywords = [\"he\", \"she\", \"they\", \"I\", \"user\", \"operator\", \"manager\", \"management\", \"team\", \"lead\", \"leader\",\n",
        "                \"inspector\", \"mechanic\", \"engineer\", \"driver\", \"pilot\", \"crew\", \"worker\", \"contractor\", \"operative\"]\n",
        "    extracted_sentences = []\n",
        "    for line in extracted_text:\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', line)\n",
        "        for sentence in sentences:\n",
        "            if any(keyword in sentence.lower() for keyword in keywords):\n",
        "                extracted_sentences.append(sentence)\n",
        "\n",
        "    combined_text = ' '.join(extracted_part + extracted_sentences)\n",
        "\n",
        "    # Load BART model and tokenizer\n",
        "    model_name = 'facebook/bart-large-cnn'\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    def summarize_text(text, max_length=150):\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_length, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    # Split the text into 1000 token chunks\n",
        "    tokens = tokenizer(combined_text, return_tensors='pt')['input_ids'][0]\n",
        "    chunk_size = 1000\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    # Summarize each chunk\n",
        "    summaries = [summarize_text(tokenizer.decode(chunk, skip_special_tokens=True)) for chunk in chunks]\n",
        "\n",
        "    # Iteratively summarize until the final combined summaries text is under 500 tokens\n",
        "    combined_summary = ' '.join(summaries)\n",
        "    while len(tokenizer(combined_summary)['input_ids']) > 500:\n",
        "        combined_summary = summarize_text(combined_summary)\n",
        "\n",
        "    return combined_summary\n",
        "\n",
        "file_path = 'your_file.pdf'\n",
        "result = extract_and_summarize(file_path)\n",
        "print(result)"
      ]
    }
  ]
}